{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "atlas_baseline_pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taguka/atlas/blob/master/atlas_baseline_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "UNW3BJeenJwe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install iterative-stratification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-YLYob5tHbYa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QKTIgdpRHx5T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "!pip3 install https://download.pytorch.org/whl/cu80/torch-1.0.0-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w0XLIC_91XHm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install pretrainedmodels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zr22BucoorXx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install kaggle\n",
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "filename = \"/content/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)\n",
        "!mkdir ~/.kaggle\n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle competitions download -c human-protein-atlas-image-classification\n",
        "!unzip -qq train.zip -d train | awk 'BEGIN {ORS=\" \"} {if(NR%500==0) print \".\"}'\n",
        "!unzip -qq test.zip -d test | awk 'BEGIN {ORS=\" \"} {if(NR%500==0) print \".\"}'\n",
        "!rm test.zip\n",
        "!rm train.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c6pjcbWvP3Y8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip -qq '/content/gdrive/My Drive/external_data.zip' -d train | awk 'BEGIN {ORS=\" \"} {if(NR%500==0) print \".\"}'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SKsldimwAzc6",
        "colab_type": "code",
        "outputId": "26ec360b-1ee8-46c4-db28-92c2fb3c662f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  gdrive  sample_data  sample_submission.csv  test  train  train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aEk78XNHwfq5",
        "colab_type": "code",
        "outputId": "84e0d7ed-0495-418a-c95f-ef20e96e9d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls '/content/gdrive/My Drive/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " atlas\t'Colab Notebooks'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g-dvBcmzmwBg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys \n",
        "import json\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np \n",
        "from torch import nn\n",
        "import torch.nn.functional as F \n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms as T\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from imgaug import augmenters as iaa\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
        "import pathlib\n",
        "import time \n",
        "import random \n",
        "import warnings\n",
        "import torchvision\n",
        "import pandas as pd \n",
        "from tqdm import tqdm \n",
        "from datetime import datetime\n",
        "from torch import nn,optim\n",
        "from collections import OrderedDict\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import lr_scheduler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from timeit import default_timer as timer\n",
        "from sklearn.metrics import f1_score\n",
        "import collections\n",
        "from torchvision import models\n",
        "from pretrainedmodels.models import bninception\n",
        "from collections import OrderedDict\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I4FH8NCbK1pL",
        "colab_type": "code",
        "outputId": "b40bcf3d-a1db-4384-dc1c-473777c87d97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "kAaeQVKmwsEd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DRIVE='/content/gdrive/My Drive/atlas'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PE4pBU8i8tYb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!ls 'external/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PDODDcNwoKuu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DefaultConfigs(object):\n",
        "    train_data = 'train/' # where is your train data\n",
        "    test_data = 'test/'   # your test data\n",
        "    weights = os.path.join(DRIVE, 'checkpoints/')\n",
        "    best_models = os.path.join(DRIVE, 'checkpoints','best_models/')\n",
        "    submit = os.path.join(DRIVE,'submit/')\n",
        "    model_name = 'bninception_bcelog'\n",
        "    num_classes = 28\n",
        "    img_weight = 512\n",
        "    img_height = 512\n",
        "    channels = 4\n",
        "    lr = 0.001\n",
        "    batch_size = 8\n",
        "    epochs = 50\n",
        "    random=50\n",
        "    fold=2\n",
        "    resume=True\n",
        "\n",
        "config = DefaultConfigs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ywhn3a6_zSCb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_net():\n",
        "    model = bninception(pretrained=\"imagenet\")\n",
        "    model.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "    model.conv1_7x7_s2 = nn.Conv2d(config.channels, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
        "    model.last_linear = nn.Sequential(\n",
        "                nn.BatchNorm1d(1024),\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(1024, config.num_classes),\n",
        "            )\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EYB6_MejEkob",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_checkpoint(state, is_best_loss,is_best_f1,fold):\n",
        "    filename = config.weights + config.model_name + os.sep +str(fold) + os.sep + \"checkpoint.pth.tar\"\n",
        "    torch.save(state, filename)\n",
        "    if is_best_loss:\n",
        "        shutil.copyfile(filename,\"%s/%s_fold_%s_model_best_loss.pth.tar\"%(config.best_models,config.model_name,str(config.fold)))\n",
        "    if is_best_f1:\n",
        "        shutil.copyfile(filename,\"%s/%s_fold_%s_model_best_f1.pth.tar\"%(config.best_models,config.model_name,str(config.fold)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P7u_u5R4VbbX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rotate(img):\n",
        "    rows = img.shape[0]\n",
        "    cols = img.shape[1]\n",
        "    angle = np.random.choice((10, 20, 30, 40, 50, 60, 70, 80, 90,180,270))\n",
        "    rotation_M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
        "    img = cv2.warpAffine(img, rotation_M, (cols, rows))\n",
        "    return img\n",
        "\n",
        "def rotate_bound(image, size):\n",
        "    #credits http://www.pyimagesearch.com/2017/01/02/rotate-images-correctly-with-opencv-and-python/\n",
        "    (h, w) = image.shape[:2]\n",
        "    (cX, cY) = (w // 2, h // 2)\n",
        "\n",
        "    angle = np.random.randint(10,180)\n",
        "\n",
        "    M = cv2.getRotationMatrix2D((cX, cY), -angle, 1.0)\n",
        "    cos = np.abs(M[0, 0])\n",
        "    sin = np.abs(M[0, 1])\n",
        "\n",
        "    # compute the new bounding dimensions of the image\n",
        "    nW = int((h * sin) + (w * cos))\n",
        "    nH = int((h * cos) + (w * sin))\n",
        "\n",
        "    # adjust the rotation matrix to take into account translation\n",
        "    M[0, 2] += (nW / 2) - cX\n",
        "    M[1, 2] += (nH / 2) - cY\n",
        "\n",
        "    output = cv2.resize(cv2.warpAffine(image, M, (nW, nH)), (size, size))\n",
        "    return output\n",
        "\n",
        "def perspective(img):\n",
        "    rows = img.shape[0]\n",
        "    cols = img.shape[1]\n",
        "\n",
        "    shrink_ratio1 = np.random.randint(low=85, high=110, dtype=int) / 100\n",
        "    shrink_ratio2 = np.random.randint(low=85, high=110, dtype=int) / 100\n",
        "\n",
        "    zero_point = rows - np.round(rows * shrink_ratio1, 0)\n",
        "    max_point_row = np.round(rows * shrink_ratio1, 0)\n",
        "    max_point_col = np.round(cols * shrink_ratio2, 0)\n",
        "\n",
        "    src = np.float32([[zero_point, zero_point], [max_point_row-1, zero_point], [zero_point, max_point_col+1], [max_point_row-1, max_point_col+1]])\n",
        "    dst = np.float32([[0, 0], [rows, 0], [0, cols], [rows, cols]])\n",
        "\n",
        "    perspective_M = cv2.getPerspectiveTransform(src, dst)\n",
        "\n",
        "    img = cv2.warpPerspective(img, perspective_M, (cols,rows))#, borderValue=mean_pix)\n",
        "    return img\n",
        "\n",
        "def shift(img):\n",
        "    rows = img.shape[0]\n",
        "    cols = img.shape[1]\n",
        "\n",
        "    shift_ratio1 = (random.random() * 2 - 1) * np.random.randint(low=3, high=15, dtype=int)\n",
        "    shift_ratio2 = (random.random() * 2 - 1) * np.random.randint(low=3, high=15, dtype=int)\n",
        "\n",
        "    shift_M = np.float32([[1,0,shift_ratio1], [0,1,shift_ratio2]])\n",
        "    img = cv2.warpAffine(img, shift_M, (cols, rows))#, borderValue=mean_pix)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IyVAlAz8x6C5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "random.seed(config.random)\n",
        "np.random.seed(config.random)\n",
        "torch.manual_seed(config.random)\n",
        "torch.cuda.manual_seed_all(config.random)\n",
        "\n",
        "# create dataset class\n",
        "class HumanDataset(Dataset):\n",
        "    def __init__(self,images_df,base_path,augument=True,mode=\"train\"):\n",
        "        if not isinstance(base_path, pathlib.Path):\n",
        "            base_path = pathlib.Path(base_path)\n",
        "        self.images_df = images_df.copy()\n",
        "        self.augument = augument\n",
        "        self.images_df.Id = self.images_df.Id.apply(lambda x:base_path / x)\n",
        "        self.mlb = MultiLabelBinarizer(classes = np.arange(0,config.num_classes))\n",
        "        self.mlb.fit(np.arange(0,config.num_classes))\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_df)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        X = self.read_images(index)\n",
        "        if not self.mode == \"test\":\n",
        "            labels = np.array(list(map(int, self.images_df.iloc[index].Target.split(' '))))\n",
        "            y  = np.eye(config.num_classes,dtype=np.float)[labels].sum(axis=0)\n",
        "        else:\n",
        "            y = str(self.images_df.iloc[index].Id.absolute())\n",
        "        if self.augument:\n",
        "            X = self.augumentor(X)\n",
        "        X = T.Compose([T.ToPILImage(),T.ToTensor(),T.Normalize([0.08069, 0.05258, 0.05487, 0.08282], [0.13704, 0.10145, 0.15313, 0.13814])])(X)\n",
        " #       X = T.Compose([T.ToPILImage(),T.ToTensor()])(X)\n",
        "        return X.float(),y\n",
        "\n",
        "\n",
        "    def read_images(self,index):\n",
        "        row = self.images_df.iloc[index]\n",
        "        filename = str(row.Id.absolute())\n",
        "        flags = cv2.IMREAD_GRAYSCALE\n",
        "        #use only rgb channels\n",
        "        if config.channels == 4:\n",
        "            images = np.zeros(shape=(512,512,4))\n",
        "        else:\n",
        "            images = np.zeros(shape=(512,512,3))\n",
        "        if filename.split('/')[-1].startswith(\"ENS\"):\n",
        "          images = cv2.resize(cv2.imread(filename+'.png',flags),(config.img_weight,config.img_height))\n",
        "        else:\n",
        "          r = cv2.imread(filename+\"_red.png\",flags)\n",
        "          g = cv2.imread(filename+\"_green.png\",flags)\n",
        "          b = cv2.imread(filename+\"_blue.png\",flags)\n",
        "          y = cv2.imread(filename+\"_yellow.png\",flags)\n",
        "          images[:,:,0] = r.astype(np.uint8) \n",
        "          images[:,:,1] = g.astype(np.uint8)\n",
        "          images[:,:,2] = b.astype(np.uint8)\n",
        "        if config.channels == 4:\n",
        "            images[:,:,3] = y.astype(np.uint8)\n",
        "        images = images.astype(np.uint8)\n",
        "        #images = np.stack(images,-1) \n",
        "        if config.img_height == 512:\n",
        "            return images\n",
        "        else:\n",
        "            return cv2.resize(images,(config.img_weight,config.img_height))\n",
        "    \n",
        "    def augumentor(self,image):\n",
        "        rnd_flip = np.random.randint(2, dtype=int)\n",
        "        rnd_rotate = np.random.randint(2, dtype=int)\n",
        "        rnd_zoom = np.random.randint(2, dtype=int)\n",
        "        rnd_shift = np.random.randint(2, dtype=int)\n",
        "\n",
        "        if (rnd_flip == 1):\n",
        "          rnd_flip = np.random.randint(3, dtype=int) - 1\n",
        "          image = cv2.flip(image, rnd_flip)\n",
        "\n",
        "        if (rnd_rotate == 1):\n",
        "          image = rotate_bound(image, config.img_weight)\n",
        "\n",
        "        if (rnd_zoom == 1):\n",
        "          image = perspective(image)\n",
        "\n",
        "        if (rnd_shift == 1):\n",
        "          image = shift(image)\n",
        "        return image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p4nx7cnwW9Si",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "    def augumentor(self,image):\n",
        "        augment_img = iaa.Sequential([\n",
        "            iaa.OneOf([\n",
        "                iaa.Affine(rotate=90),\n",
        "                iaa.Affine(rotate=180),\n",
        "                iaa.Affine(rotate=270),\n",
        "                iaa.Affine(shear=(-16, 16)),\n",
        "                iaa.Fliplr(0.5),\n",
        "                iaa.Flipud(0.5),\n",
        "                \n",
        "            ])], random_order=True)\n",
        "        \n",
        "        image_aug = augment_img.augment_image(image)\n",
        "        return image_aug"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gf1t-BgOzq_e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Logger(object):\n",
        "    def __init__(self):\n",
        "      self.terminal = sys.stdout  #stdout\n",
        "      self.file = None\n",
        "\n",
        "    def open(self, file, mode=None):\n",
        "      if mode is None: \n",
        "        mode ='w'\n",
        "      self.file = open(file, mode)\n",
        "\n",
        "    def write(self, message, is_terminal=1, is_file=1 ):\n",
        "      if '\\r' in message: \n",
        "        is_file=0\n",
        "\n",
        "      if is_terminal == 1: \n",
        "        self.terminal.write(message)\n",
        "        self.terminal.flush()\n",
        "          #time.sleep(1)\n",
        "\n",
        "      if is_file == 1:\n",
        "        self.file.write(message)\n",
        "        self.file.flush()\n",
        "\n",
        "    def flush(self):\n",
        "        # this flush method is needed for python 3 compatibility.\n",
        "        # this handles the flush command by doing nothing.\n",
        "        # you might want to specify some extra behavior here.\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ns-biH1zz94e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# evaluate meters\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "en51O5NN0Cp5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25,gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        '''Focal loss.\n",
        "        Args:\n",
        "          x: (tensor) sized [N,D].\n",
        "          y: (tensor) sized [N,].\n",
        "        Return:\n",
        "          (tensor) focal loss.\n",
        "        '''\n",
        "        t = Variable(y).cuda()  # [N,20]\n",
        "\n",
        "        p = x.sigmoid()\n",
        "        pt = p*t + (1-p)*(1-t)         # pt = p if t > 0 else 1-p\n",
        "        w = self.alpha*t + (1-self.alpha)*(1-t)  # w = alpha if t > 0 else 1-alpha\n",
        "        w = w * (1-pt).pow(self.gamma)\n",
        "        return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EAVQaUbw0JAy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_learning_rate(optimizer):\n",
        "    lr=[]\n",
        "    for param_group in optimizer.param_groups:\n",
        "       lr +=[ param_group['lr'] ]\n",
        "\n",
        "    #assert(len(lr)==1) #we support only one param_group\n",
        "    lr = lr[0]\n",
        "\n",
        "    return lr\n",
        "\n",
        "def time_to_str(t, mode='min'):\n",
        "    if mode=='min':\n",
        "        t  = int(t)/60\n",
        "        hr = t//60\n",
        "        min = t%60\n",
        "        return '%2d hr %02d min'%(hr,min)\n",
        "\n",
        "    elif mode=='sec':\n",
        "        t   = int(t)\n",
        "        min = t//60\n",
        "        sec = t%60\n",
        "        return '%2d min %02d sec'%(min,sec)\n",
        "\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lk9tMF7Sywxx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(train_loader,model,criterion,optimizer,epoch,valid_loss,best_results,start):\n",
        "    losses = AverageMeter()\n",
        "    f1 = AverageMeter()\n",
        "    model.train()\n",
        "      \n",
        "    for i,(images,target) in enumerate(train_loader):\n",
        "      images = images.cuda(non_blocking=True)\n",
        "      target = torch.from_numpy(np.array(target)).float().cuda(non_blocking=True)\n",
        "      # compute output\n",
        "      output = model(images)\n",
        "      loss = criterion(output,target)\n",
        "      losses.update(loss.item(),images.size(0))\n",
        "\n",
        "      f1_batch = f1_score(target.cpu(),output.sigmoid().cpu() > 0.2,average='macro')    \n",
        "      f1.update(f1_batch,images.size(0))\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      print('\\r',end='',flush=True)\n",
        "      message = '%s %5.1f %6.1f         |         %0.3f  %0.3f           |         %0.3f  %0.4f         |         %s  %s    | %s' % (\\\n",
        "                \"train\", i/len(train_loader), epoch,\n",
        "                losses.avg, f1.avg, \n",
        "                valid_loss[0], valid_loss[1], \n",
        "                str(best_results[0])[:8],str(best_results[1])[:8],\n",
        "                time_to_str((timer() - start),'min'))\n",
        "      print(message , end='',flush=True)\n",
        "    log.write(\"\\n\")\n",
        "#    log.write(message,is_terminal=0)\n",
        "#    log.write(\"\\n\")\n",
        "    return [losses.avg,f1.avg]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wjfiwjVXz2Ma",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 2. evaluate function\n",
        "def evaluate(val_loader,model,criterion,epoch,train_loss,best_results,start):\n",
        "    # only meter loss and f1 score\n",
        "    losses = AverageMeter()\n",
        "    f1 = AverageMeter()\n",
        "    # switch mode for evaluation\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (images,target) in enumerate(val_loader):\n",
        "            images_var = images.cuda(non_blocking=True)\n",
        "            target = torch.from_numpy(np.array(target)).float().cuda(non_blocking=True)\n",
        "            #image_var = Variable(images).cuda()\n",
        "            #target = Variable(torch.from_numpy(np.array(target)).long()).cuda()\n",
        "            output = model(images_var)\n",
        "            loss = criterion(output,target)\n",
        "            losses.update(loss.item(),images_var.size(0))\n",
        "            f1_batch = f1_score(target.cpu(),output.sigmoid().cpu().data.numpy() > 0.2,average='macro')\n",
        "            f1.update(f1_batch,images_var.size(0))\n",
        "            print('\\r',end='',flush=True)\n",
        "            message = '%s   %5.1f %6.1f         |         %0.3f  %0.3f           |         %0.3f  %0.4f         |         %s  %s    | %s' % (\\\n",
        "                    \"val\", i , epoch,                    \n",
        "                    train_loss[0], train_loss[1], \n",
        "                    losses.avg, f1.avg,\n",
        "                    str(best_results[0])[:8],str(best_results[1])[:8],\n",
        "                    time_to_str((timer() - start),'min'))\n",
        "\n",
        "            print(message, end='',flush=True)\n",
        "        log.write(\"\\n\")\n",
        "#        log.write(message,is_terminal=0)\n",
        "#        log.write(\"\\n\")\n",
        "        \n",
        "    return [losses.avg,f1.avg]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vqcjjPYj0Q2c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 3. test model on public dataset and save the probability matrix\n",
        "\n",
        "def test(test_loader,model,folds):\n",
        "    sample_submission_df = pd.read_csv('sample_submission.csv')\n",
        "    #3.1 confirm the model converted to cuda\n",
        "    filenames,labels ,submissions= [],[],[]\n",
        "    prob_labels=[]\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    submit_results = []\n",
        "    for i,(input,filepath) in tqdm(enumerate(test_loader),position=0):\n",
        "        #3.2 change everything to cuda and get only basename\n",
        "        filepath = [os.path.basename(x) for x in filepath]\n",
        "        with torch.no_grad():\n",
        "            image_var = input.cuda(non_blocking=True)\n",
        "            y_pred = model(image_var)\n",
        "            label = y_pred.sigmoid().cpu().data.numpy()\n",
        "            prob_labels.append(label[0])          \n",
        "            labels.append(label > 0.2)\n",
        "            filenames.extend(filepath)\n",
        "    sub_labels=pd.DataFrame(prob_labels)\n",
        "    sub_labels['Id']=filenames\n",
        "    sub_labels.to_csv(os.path.join(DRIVE,'sub_labels_%s.csv'%str(config.fold)))\n",
        "    \n",
        "    for row in np.concatenate(labels):\n",
        "        subrow = ' '.join(list([str(i) for i in np.nonzero(row)[0]]))\n",
        "        submissions.append(subrow)\n",
        "    sample_submission_df['Predicted'] = submissions\n",
        "    sample_submission_df.to_csv(os.path.join(DRIVE,'submit/%s_bestloss_submission.csv'%config.model_name), index=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l_CdbXVF01FQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 4. main function\n",
        "def main():\n",
        "    fold = config.fold\n",
        "    # 4.1 mkdirs\n",
        "    if not os.path.exists(config.submit):\n",
        "        os.makedirs(config.submit)\n",
        "    if not os.path.exists(config.weights + config.model_name + os.sep +str(fold)):\n",
        "        os.makedirs(config.weights + config.model_name + os.sep +str(fold))\n",
        "    if not os.path.exists(config.best_models):\n",
        "        os.mkdir(config.best_models)\n",
        "    if not os.path.exists(os.path.join(DRIVE, 'logs/')):\n",
        "        os.mkdir(os.path.join(DRIVE, 'logs/'))\n",
        "    \n",
        "    # 4.2 get model\n",
        "    model = get_net()\n",
        "    model.cuda()\n",
        "\n",
        "    # criterion\n",
        "#    optimizer = optim.SGD(model.parameters(),lr = config.lr,momentum=0.9,weight_decay=1e-4)\n",
        "    optimizer = optim.Adam(model.parameters(),lr = config.lr,weight_decay=1e-7)\n",
        "    criterion = nn.BCEWithLogitsLoss().cuda()\n",
        "    #criterion = FocalLoss().cuda()\n",
        "    #criterion = F1Loss().cuda()\n",
        "    start_epoch = 0\n",
        "    best_loss = 999\n",
        "    best_f1 = 0\n",
        "    best_results = [np.inf,0]\n",
        "    val_metrics = [np.inf,0]\n",
        "    resume = config.resume\n",
        "    all_files = pd.read_csv('train.csv')\n",
        "    train_df_orig=all_files.copy()    \n",
        "    lows = [15,15,15,8,9,10,8,9,10,8,9,10,17,20,24,26,15,27,15,20,24,17,8,15,27,27,27]\n",
        "    for i in lows:\n",
        "      target = str(i)\n",
        "      indicies = train_df_orig.loc[train_df_orig['Target'] == target].index\n",
        "      all_files = pd.concat([all_files,train_df_orig.loc[indicies]], ignore_index=True)\n",
        "      indicies = train_df_orig.loc[train_df_orig['Target'].str.startswith(target+\" \")].index\n",
        "      all_files = pd.concat([all_files,train_df_orig.loc[indicies]], ignore_index=True)\n",
        "      indicies = train_df_orig.loc[train_df_orig['Target'].str.endswith(\" \"+target)].index\n",
        "      all_files = pd.concat([all_files,train_df_orig.loc[indicies]], ignore_index=True)\n",
        "      indicies = train_df_orig.loc[train_df_orig['Target'].str.contains(\" \"+target+\" \")].index\n",
        "      all_files = pd.concat([all_files,train_df_orig.loc[indicies]], ignore_index=True)\n",
        "        \n",
        "    #print(all_files)\n",
        "    test_files = pd.read_csv('sample_submission.csv')  \n",
        "    all_files['Target_binary']=all_files['Target'].apply(lambda x: list(np.eye(28)[list(map(int,x.split(' ')))].sum(axis=0).astype(int)) )\n",
        "    all_files['class']=all_files['Target'].apply(lambda x: int(x.split(' ')[np.random.randint(0,len(x.split(' ')))]))\n",
        "    lst_class=[]\n",
        " \n",
        "    for class_id in all_files['class']:\n",
        "      lst_class.append(class_id)\n",
        "    counter_class=collections.Counter(lst_class)   \n",
        "    class_sample_counts=[counter_class[key] for key in sorted(counter_class.keys())]\n",
        "    class_weights = 1./torch.Tensor(class_sample_counts)\n",
        "\n",
        "    msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=config.random)\n",
        "    X=np.array([item for item in all_files.Id])\n",
        "    y=np.array([item for item in all_files.Target_binary])\n",
        "\n",
        "    for train_index, test_index in msss.split(X, y):\n",
        "      train_data_list=all_files.iloc[train_index]\n",
        "      val_data_list=all_files.iloc[test_index]\n",
        "    # load dataset\n",
        "    train_gen = HumanDataset(train_data_list,config.train_data,mode=\"train\")\n",
        "    val_gen = HumanDataset(val_data_list,config.train_data,augument=False,mode=\"train\")\n",
        "    test_gen = HumanDataset(test_files,config.test_data,augument=False,mode=\"test\")\n",
        "    \n",
        "    # compute weight for all the samples in the dataset\n",
        "    # samples_weights contain the probability for each example in dataset to be sampled  \n",
        "    train_targets = [sample for sample in train_gen.images_df['class']]\n",
        "    train_samples_weight = [class_weights[int(class_id)] for class_id in train_targets]\n",
        "    train_sampler = torch.utils.data.sampler.WeightedRandomSampler(train_samples_weight, len(train_gen))\n",
        "\n",
        "    val_targets = [sample for sample in val_gen.images_df['class']]\n",
        "    val_samples_weight = [class_weights[int(class_id)] for class_id in val_targets]\n",
        "    val_sampler = torch.utils.data.sampler.WeightedRandomSampler(val_samples_weight, len(val_gen))\n",
        "\n",
        "    train_loader = DataLoader(train_gen,batch_size=config.batch_size,sampler=train_sampler,shuffle=False, pin_memory=True, drop_last=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_gen,batch_size=config.batch_size,sampler=val_sampler, shuffle=False,pin_memory=True,drop_last=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_gen,1,shuffle=False,pin_memory=True,num_workers=4)\n",
        "    scheduler = lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.1)\n",
        "    start = timer()\n",
        "    if resume:\n",
        "      checkpoint = torch.load(os.path.join(DRIVE,\"checkpoints/bninception_bcelog/%s/checkpoint.pth.tar\"%str(fold)))\n",
        "      model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "      start_epoch = checkpoint['epoch']\n",
        "      best_loss = checkpoint['best_loss']\n",
        "      optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "    best_model = torch.load(\"%s%s_fold_%s_model_best_loss.pth.tar\"%(config.best_models,config.model_name,str(fold)))\n",
        "    model.load_state_dict(best_model[\"state_dict\"])\n",
        "    test(test_loader,model,config.fold)\n",
        "\"\"\"\n",
        "    for epoch in range(start_epoch,config.epochs):    \n",
        "      scheduler.step(epoch)\n",
        "      # train\n",
        "      lr = get_learning_rate(optimizer)\n",
        "      train_metrics = train(train_loader,model,criterion,optimizer,epoch,val_metrics,best_results,start)\n",
        "      # val\n",
        "      val_metrics = evaluate(val_loader,model,criterion,epoch,train_metrics,best_results,start)\n",
        "      # check results \n",
        "      is_best_loss = val_metrics[0] < best_results[0]\n",
        "      best_results[0] = min(val_metrics[0],best_results[0])\n",
        "      is_best_f1 = val_metrics[1] > best_results[1]\n",
        "      best_results[1] = max(val_metrics[1],best_results[1])   \n",
        "      # save model\n",
        "      save_checkpoint({\n",
        "                    \"epoch\":epoch + 1,\n",
        "                    \"model_name\":config.model_name,\n",
        "                    \"state_dict\":model.state_dict(),\n",
        "                    \"best_loss\":best_results[0],\n",
        "                    \"optimizer\":optimizer.state_dict(),\n",
        "                    \"fold\":fold,\n",
        "                    \"best_f1\":best_results[1],\n",
        "      },is_best_loss,is_best_f1,fold)\n",
        "      # print logs\n",
        "      print('\\r',end='',flush=True)\n",
        "      log.write('%s  %5.1f %6.1f         |         %0.3f  %0.3f           |         %0.3f  %0.4f         |         %s  %s    | %s' % (\\\n",
        "                \"best\", epoch, epoch,                    \n",
        "                train_metrics[0], train_metrics[1], \n",
        "                val_metrics[0], val_metrics[1],\n",
        "                str(best_results[0])[:8],str(best_results[1])[:8],\n",
        "                time_to_str((timer() - start),'min'))\n",
        "               )\n",
        "      log.write(\"\\n\")\n",
        "      time.sleep(0.01)\n",
        "    \n",
        "#    best_model = torch.load(\"%s%s_fold_%s_model_best_loss.pth.tar\"%(config.best_models,config.model_name,str(fold)))\n",
        "#    best_model = torch.load(\"checkpoints/bninception_bcelog/0/checkpoint.pth.tar\")#    model.load_state_dict(best_model[\"state_dict\"])\n",
        "#    test(test_loader,model,fold)\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "flQTlHkEqMS6",
        "colab_type": "code",
        "outputId": "a7170f55-6231-44a3-9a79-e38f10d668e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  sample_data\t\t test\t   train\n",
            "gdrive\t  sample_submission.csv  test.txt  train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KMwrclMm08-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ec356ce6-b591-4647-b593-e42eedee96d5"
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
        "  torch.backends.cudnn.benchmark = True\n",
        "  warnings.filterwarnings('ignore')\n",
        "\n",
        "  if not os.path.exists(os.path.join(DRIVE,'logs/')):\n",
        "    os.mkdir(os.path.join(DRIVE,'logs/'))\n",
        "\n",
        "  log = Logger()\n",
        "  log.open(os.path.join(DRIVE,'logs',\"%s_log_train.txt\"%config.model_name),'a')\n",
        "  log.write(\"\\n----------------------------------------------- [START %s] %s\\n\\n\" % (datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '-' * 51))\n",
        "  log.write('                           |------------ Train -------------|----------- Valid -------------|----------Best Results---------|------------|\\n')\n",
        "  log.write('mode     iter     epoch    |         loss   f1_macro        |         loss   f1_macro       |         loss   f1_macro       | time       |\\n')\n",
        "  log.write('-------------------------------------------------------------------------------------------------------------------------------\\n')\n",
        "  \n",
        "  main()"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------- [START 2019-01-03 17:23:17] ---------------------------------------------------\n",
            "\n",
            "                           |------------ Train -------------|----------- Valid -------------|----------Best Results---------|------------|\n",
            "mode     iter     epoch    |         loss   f1_macro        |         loss   f1_macro       |         loss   f1_macro       | time       |\n",
            "-------------------------------------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "11702it [09:42, 20.10it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}